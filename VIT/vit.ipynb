{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8b212267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0cdf7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "38c38b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dirs = [Path(\"./\"), Path(\"../\")]\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    # 0.1307 is the mean of the MNIST dataset, 0.3081 is the standard deviation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)), \n",
    "])\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    if (data_dir / \"MNIST\").exists():\n",
    "        raw_train_data = datasets.MNIST(data_dir, train=True, transform=tf)\n",
    "        raw_test_data = datasets.MNIST(data_dir, train=False, transform=tf)\n",
    "        break\n",
    "else:\n",
    "    raw_train_data = datasets.MNIST(\"./\", train=True, download=True, transform=tf)\n",
    "    raw_test_data = datasets.MNIST(\"./\", train=False, download=True, transform=tf)\n",
    "    \n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6288cd9",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1965999",
   "metadata": {},
   "source": [
    "### Patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "61d589a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    divide image into non-overlapping patches\n",
    "    (C, H, W) -> (N, P*P*C) where N = (H/P)*(W/P)\n",
    "    note: H and W must be divisible by P\n",
    "    \"\"\"\n",
    "    if x.size(-1) % patch_size != 0 or x.size(-2) % patch_size != 0:\n",
    "        raise ValueError(\"Height and Width must be divisible by patch_size\")\n",
    "    N = (x.size(-2)//patch_size) * (x.size(-1) // patch_size)\n",
    "    x = x.unfold(1, patch_size,patch_size).unfold(2, patch_size,patch_size)  # (C, H/P, W/P, P, P)\n",
    "    x = x.permute(1, 2, 0, 3, 4)  # (H/P, W/P, C, P, P)\n",
    "\n",
    "    return x.reshape(N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "48b3a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, dataset, patch_size):\n",
    "        self.labels = []\n",
    "        self.img_patches = []\n",
    "        for img, label in dataset:\n",
    "            self.labels.append(label)\n",
    "            self.img_patches.append(patch(img, patch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.img_patches[idx], self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "daa23644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedPatchDataset(Dataset):\n",
    "    def __init__(self, dataset, patch_size):\n",
    "        self.patch_size = patch_size\n",
    "        self.labels = []\n",
    "        self.patched_features = []\n",
    "        for img, label in dataset:\n",
    "            self.labels.append(label)\n",
    "            self.patched_features.append(self._shifted_patch(img))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.patched_features[idx], self.labels[idx])\n",
    "\n",
    "    def _shifted_patch(self, x):\n",
    "        # Shifting and concat\n",
    "        # (C, H, W) -> (5, C, H, W)\n",
    "        patch_size = self.patch_size\n",
    "        p = self.patch_size / 4\n",
    "        translates = [[0, 0], [p, p],[p,-p],[-p,-p],[-p,p]]\n",
    "        shifted = torch.stack(\n",
    "            [ v2.functional.affine(x, 0, translate, 1, [0.0], 0) for translate in translates ],\n",
    "            dim = 0\n",
    "        )\n",
    "\n",
    "        N = (shifted.size(-2)//patch_size) * (shifted.size(-1) // patch_size)\n",
    "\n",
    "        # (5, C, H, W) -> (N, P*P*C*5)\n",
    "        x = shifted.unfold(2,patch_size,patch_size).unfold(3,patch_size,patch_size) # (5, C, H/P, W/P, P, P)\n",
    "        x = x.permute(2, 3, 0, 1, 4, 5) # (H/P, W/P, 5, C, P, P)\n",
    "\n",
    "        return x.reshape(N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "43621f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAafElEQVR4nO3dDXAUZZ7H8f8AIRBIgiGQlyVgeBOXl3giYgrEuOQSsJYCpDxQtwo8DwoEdyG+cLEUxHUrilesC4dwt7USrVJAtoSslHKFYJJlTbAAWYpbRYJRwpIEwUoCQUJI+uppLjGjAfYZEv6T6e+nqmvSM/2nm05nfvN0P/2Mz3EcRwAAuME63egVAgBgEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQ0UWCTGNjo5w8eVIiIyPF5/Npbw4AwJIZ3+Ds2bOSmJgonTp16jgBZMInKSlJezMAANeprKxM+vXr13ECyLR8jPFyn3SRMO3NAQBYuiT1skfeb34/v+EBtHbtWnnllVekoqJCUlJSZM2aNXLnnXdes67ptJsJny4+AggAOpz/H2H0WpdR2qUTwubNmyUrK0uWL18uBw4ccAMoMzNTTp061R6rAwB0QO0SQKtWrZK5c+fKI488Ij/96U9l/fr1EhERIa+//np7rA4A0AG1eQBdvHhR9u/fL+np6d+vpFMnd76oqOhHy9fV1UlNTY3fBAAIfW0eQKdPn5aGhgaJi4vze97Mm+tBP5STkyPR0dHNEz3gAMAb1G9Ezc7Olurq6ubJdNsDAIS+Nu8FFxsbK507d5bKykq/5818fHz8j5YPDw93JwCAt7R5C6hr164yevRo2bVrl9/oBmY+NTW1rVcHAOig2uU+INMFe/bs2XLHHXe49/68+uqrUltb6/aKAwCg3QJo5syZ8s0338iyZcvcjge33Xab7Nix40cdEwAA3uVzzKhxQcR0wza94dJkKiMhAEAHdMmpl3zJczuWRUVFBW8vOACANxFAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQ0UVntUBw8nWx/5Po3CdWgtWRJ28OqK4hotG6ZsCgU9Y1EY/5rGsqVnW1rjlwx2YJxOmGWuuasVuesK4ZnFUsXkQLCACgggACAIRGAD3//PPi8/n8pmHDhrX1agAAHVy7XAMaPny4fPjhh9+vJIDz6gCA0NYuyWACJz4+vj3+aQBAiGiXa0BHjx6VxMREGThwoDz88MNy/PjxKy5bV1cnNTU1fhMAIPS1eQCNHTtWcnNzZceOHbJu3TopLS2Vu+++W86ePdvq8jk5ORIdHd08JSUltfUmAQC8EECTJ0+WBx54QEaNGiWZmZny/vvvS1VVlbzzzjutLp+dnS3V1dXNU1lZWVtvEgAgCLV774BevXrJ0KFDpaSkpNXXw8PD3QkA4C3tfh/QuXPn5NixY5KQkNDeqwIAeDmAnnzySSkoKJCvvvpKPv74Y5k+fbp07txZHnzwwbZeFQCgA2vzU3AnTpxww+bMmTPSp08fGT9+vBQXF7s/AwDQbgG0adOmtv4nEaQ63zrEusYJD7OuOXlPL+ua7+6yH0TSiIm2r/tzSmADXYaaD85HWte8/J+TrGv2jnzbuqa0/jsJxEuV/2xdk/hnJ6B1eRFjwQEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAAjNL6RD8GtIuz2gulW5a61rhoZ1DWhduLHqnQbrmmVr5ljXdKm1H7gzdcsi65rIv1+SQISfth/ENGLf3oDW5UW0gAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKhgNGxJ+5GRAdfsvJFnXDA2rDGhdoeaJ8rusa748F2tdkzvojxKI6kb7UarjVn8socZ+L8AGLSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqGIwUcqm8IqC6NS8/YF3zm0m11jWdD/W0rvnrY2vkRnnx9CjrmpL0COuahqpy65qHUh+TQHz1S/uaZPlrQOuCd9ECAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoILBSBGwmA1F1jV93uttXdNw5lvrmuEj/lUC8b8TXreu+dN/32Nd07fqY7kRfEWBDRCabP+rBazRAgIAqCCAAAAdI4AKCwtlypQpkpiYKD6fT7Zt2+b3uuM4smzZMklISJDu3btLenq6HD16tC23GQDgxQCqra2VlJQUWbt2bauvr1y5UlavXi3r16+XvXv3So8ePSQzM1MuXLjQFtsLAPBqJ4TJkye7U2tM6+fVV1+VZ599VqZOneo+9+abb0pcXJzbUpo1a9b1bzEAICS06TWg0tJSqaiocE+7NYmOjpaxY8dKUVHr3Wrq6uqkpqbGbwIAhL42DSATPoZp8bRk5pte+6GcnBw3pJqmpKSkttwkAECQUu8Fl52dLdXV1c1TWVmZ9iYBADpaAMXHx7uPlZWVfs+b+abXfig8PFyioqL8JgBA6GvTAEpOTnaDZteuXc3PmWs6pjdcampqW64KAOC1XnDnzp2TkpISv44HBw8elJiYGOnfv78sXrxYXnzxRRkyZIgbSM8995x7z9C0adPaetsBAF4KoH379sm9997bPJ+VleU+zp49W3Jzc+Xpp5927xWaN2+eVFVVyfjx42XHjh3SrVu3tt1yAECH5nPMzTtBxJyyM73h0mSqdPGFaW8OOqgv/mtMYHU/X29d88jXE61rvhl/1rpGGhvsawAFl5x6yZc8t2PZ1a7rq/eCAwB4EwEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEACgY3wdA9AR3Lr0i4DqHhlpP7L1hgHffwHjP+qeBxZa10RuLrauAYIZLSAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqGIwUIamhqjqgujMLbrWuOf6n76xr/v3FN61rsv9lunWN82m0BCLpN0X2RY4T0LrgXbSAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqGAwUqCFxr9+Zl0za8VT1jVvLf8P65qDd9kPYCp3SUCG91hkXTPk9+XWNZe+/Mq6BqGDFhAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVPsdxHAkiNTU1Eh0dLWkyVbr4wrQ3B2gXzrjbrGuiXjphXbNx4P/IjTLso3+zrrllRbV1TcPRL61rcGNdcuolX/KkurpaoqKirrgcLSAAgAoCCADQMQKosLBQpkyZIomJieLz+WTbtm1+r8+ZM8d9vuU0adKkttxmAIAXA6i2tlZSUlJk7dq1V1zGBE55eXnztHHjxuvdTgCA178RdfLkye50NeHh4RIfH3892wUACHHtcg0oPz9f+vbtK7fccossWLBAzpw5c8Vl6+rq3J5vLScAQOhr8wAyp9/efPNN2bVrl7z88stSUFDgtpgaGhpaXT4nJ8ftdt00JSUltfUmAQBC4RTctcyaNav555EjR8qoUaNk0KBBbqto4sSJP1o+OztbsrKymudNC4gQAoDQ1+7dsAcOHCixsbFSUlJyxetF5kallhMAIPS1ewCdOHHCvQaUkJDQ3qsCAITyKbhz5875tWZKS0vl4MGDEhMT404rVqyQGTNmuL3gjh07Jk8//bQMHjxYMjMz23rbAQBeCqB9+/bJvffe2zzfdP1m9uzZsm7dOjl06JC88cYbUlVV5d6smpGRIb/+9a/dU20AADRhMFKgg+gc19e65uTMwQGta+/S31nXdArgjP7DpRnWNdXjr3xbB4IDg5ECAIIaAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQACA0vpIbQPtoqDxlXRO32r7GuPD0JeuaCF9X65rf37zduubn0xdb10Rs3Wtdg/ZHCwgAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKBiMFFDSOv8265tgD3axrRtz2lQQikIFFA7Hm23+yronI29cu24IbjxYQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQxGCrTgu2OEdc0Xv7QfuPP3496wrpnQ7aIEszqn3rqm+Ntk+xU1ltvXICjRAgIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCwUgR9LokD7CuOfZIYkDren7mJuuaGT1PS6h5pvIO65qC391lXXPTG0XWNQgdtIAAACoIIABA8AdQTk6OjBkzRiIjI6Vv374ybdo0OXLkiN8yFy5ckIULF0rv3r2lZ8+eMmPGDKmsrGzr7QYAeCmACgoK3HApLi6WnTt3Sn19vWRkZEhtbW3zMkuWLJH33ntPtmzZ4i5/8uRJuf/++9tj2wEAXumEsGPHDr/53NxctyW0f/9+mTBhglRXV8sf/vAHefvtt+VnP/uZu8yGDRvk1ltvdUPrrrvsL1ICAELTdV0DMoFjxMTEuI8miEyrKD09vXmZYcOGSf/+/aWoqPXeLnV1dVJTU+M3AQBCX8AB1NjYKIsXL5Zx48bJiBEj3OcqKiqka9eu0qtXL79l4+Li3NeudF0pOjq6eUpKSgp0kwAAXgggcy3o8OHDsmmT/X0TLWVnZ7stqaaprKzsuv49AEAI34i6aNEi2b59uxQWFkq/fv2an4+Pj5eLFy9KVVWVXyvI9IIzr7UmPDzcnQAA3mLVAnIcxw2frVu3yu7duyU5Odnv9dGjR0tYWJjs2rWr+TnTTfv48eOSmpradlsNAPBWC8icdjM93PLy8tx7gZqu65hrN927d3cfH330UcnKynI7JkRFRcnjjz/uhg894AAAAQfQunXr3Me0tDS/501X6zlz5rg///a3v5VOnTq5N6CaHm6ZmZny2muv2awGAOABPsecVwsiphu2aUmlyVTp4gvT3hxcRZeb+1vXVI9OsK6Z+YL//Wf/iPm9vpRQ80S5/VmEotfsBxU1YnI/sS9qbAhoXQg9l5x6yZc8t2OZORN2JYwFBwBQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBADoON+IiuDVJaH1b569mm9f7xHQuhYkF1jXPBhZKaFm0d/HW9ccWHebdU3sHw9b18ScLbKuAW4UWkAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUMBjpDXIx8w77miXfWtc8M/h965qM7rUSaiobvguobsKfnrCuGfbs59Y1MVX2g4Q2WlcAwY0WEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUMRnqDfDXNPuu/GLlFgtnaqkHWNb8ryLCu8TX4rGuGvVgqgRhSude6piGgNQGgBQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAECFz3EcR4JITU2NREdHS5pMlS6+MO3NAQBYuuTUS77kSXV1tURFRV1xOVpAAAAVBBAAIPgDKCcnR8aMGSORkZHSt29fmTZtmhw5csRvmbS0NPH5fH7T/Pnz23q7AQBeCqCCggJZuHChFBcXy86dO6W+vl4yMjKktrbWb7m5c+dKeXl587Ry5cq23m4AgJe+EXXHjh1+87m5uW5LaP/+/TJhwoTm5yMiIiQ+Pr7tthIAEHKu6xqQ6eFgxMTE+D3/1ltvSWxsrIwYMUKys7Pl/PnzV/w36urq3J5vLScAQOizagG11NjYKIsXL5Zx48a5QdPkoYcekgEDBkhiYqIcOnRIli5d6l4nevfdd694XWnFihWBbgYAwGv3AS1YsEA++OAD2bNnj/Tr1++Ky+3evVsmTpwoJSUlMmjQoFZbQGZqYlpASUlJ3AcEACF+H1BALaBFixbJ9u3bpbCw8KrhY4wdO9Z9vFIAhYeHuxMAwFusAsg0lh5//HHZunWr5OfnS3Jy8jVrDh486D4mJCQEvpUAAG8HkOmC/fbbb0teXp57L1BFRYX7vBk6p3v37nLs2DH39fvuu0969+7tXgNasmSJ20Nu1KhR7fV/AACE+jUgc1NpazZs2CBz5syRsrIy+cUvfiGHDx927w0y13KmT58uzz777FXPA7bEWHAA0LG1yzWga2WVCRxzsyoAANfCWHAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABVdJMg4juM+XpJ6kcs/AgA6EPf9u8X7eYcJoLNnz7qPe+R97U0BAFzn+3l0dPQVX/c514qoG6yxsVFOnjwpkZGR4vP5/F6rqamRpKQkKSsrk6ioKPEq9sNl7IfL2A+XsR+CZz+YWDHhk5iYKJ06deo4LSCzsf369bvqMmanevkAa8J+uIz9cBn74TL2Q3Dsh6u1fJrQCQEAoIIAAgCo6FABFB4eLsuXL3cfvYz9cBn74TL2w2Xsh463H4KuEwIAwBs6VAsIABA6CCAAgAoCCACgggACAKjoMAG0du1aufnmm6Vbt24yduxY+eSTT8Rrnn/+eXd0iJbTsGHDJNQVFhbKlClT3Luqzf9527Ztfq+bfjTLli2ThIQE6d69u6Snp8vRo0fFa/thzpw5Pzo+Jk2aJKEkJydHxowZ446U0rdvX5k2bZocOXLEb5kLFy7IwoULpXfv3tKzZ0+ZMWOGVFZWitf2Q1pa2o+Oh/nz50sw6RABtHnzZsnKynK7Fh44cEBSUlIkMzNTTp06JV4zfPhwKS8vb5727Nkjoa62ttb9nZsPIa1ZuXKlrF69WtavXy979+6VHj16uMeHeSPy0n4wTOC0PD42btwooaSgoMANl+LiYtm5c6fU19dLRkaGu2+aLFmyRN577z3ZsmWLu7wZ2uv+++8Xr+0HY+7cuX7Hg/lbCSpOB3DnnXc6CxcubJ5vaGhwEhMTnZycHMdLli9f7qSkpDheZg7ZrVu3Ns83NjY68fHxziuvvNL8XFVVlRMeHu5s3LjR8cp+MGbPnu1MnTrV8ZJTp065+6KgoKD5dx8WFuZs2bKleZnPPvvMXaaoqMjxyn4w7rnnHudXv/qVE8yCvgV08eJF2b9/v3tapeV4cWa+qKhIvMacWjKnYAYOHCgPP/ywHD9+XLystLRUKioq/I4PMwaVOU3rxeMjPz/fPSVzyy23yIIFC+TMmTMSyqqrq93HmJgY99G8V5jWQMvjwZym7t+/f0gfD9U/2A9N3nrrLYmNjZURI0ZIdna2nD9/XoJJ0A1G+kOnT5+WhoYGiYuL83vezH/++efiJeZNNTc3131zMc3pFStWyN133y2HDx92zwV7kQkfo7Xjo+k1rzCn38yppuTkZDl27Jg888wzMnnyZPeNt3PnzhJqzMj5ixcvlnHjxrlvsIb5nXft2lV69erlmeOhsZX9YDz00EMyYMAA9wProUOHZOnSpe51onfffVeCRdAHEL5n3kyajBo1yg0kc4C988478uijj6puG/TNmjWr+eeRI0e6x8igQYPcVtHEiRMl1JhrIObDlxeugwayH+bNm+d3PJhOOuY4MB9OzHERDIL+FJxpPppPbz/sxWLm4+PjxcvMp7yhQ4dKSUmJeFXTMcDx8WPmNK35+wnF42PRokWyfft2+eijj/y+vsX8zs1p+6qqKk8cD4uusB9aYz6wGsF0PAR9AJnm9OjRo2XXrl1+TU4zn5qaKl527tw599OM+WTjVeZ0k3ljaXl8mC/kMr3hvH58nDhxwr0GFErHh+l/Yd50t27dKrt373Z//y2Z94qwsDC/48GcdjLXSkPpeHCusR9ac/DgQfcxqI4HpwPYtGmT26spNzfX+dvf/ubMmzfP6dWrl1NRUeF4yRNPPOHk5+c7paWlzl/+8hcnPT3diY2NdXvAhLKzZ886n376qTuZQ3bVqlXuz19//bX7+ksvveQeD3l5ec6hQ4fcnmDJycnOd99953hlP5jXnnzySbenlzk+PvzwQ+f22293hgwZ4ly4cMEJFQsWLHCio6Pdv4Py8vLm6fz5883LzJ8/3+nfv7+ze/duZ9++fU5qaqo7hZIF19gPJSUlzgsvvOD+/83xYP42Bg4c6EyYMMEJJh0igIw1a9a4B1XXrl3dbtnFxcWO18ycOdNJSEhw98FPfvITd94caKHuo48+ct9wfziZbsdNXbGfe+45Jy4uzv2gMnHiROfIkSOOl/aDeePJyMhw+vTp43ZDHjBggDN37tyQ+5DW2v/fTBs2bGhexnzweOyxx5ybbrrJiYiIcKZPn+6+OXtpPxw/ftwNm5iYGPdvYvDgwc5TTz3lVFdXO8GEr2MAAKgI+mtAAIDQRAABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQDT8H4W4/An26RX9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x100 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test():\n",
    "    inp = raw_train_data[0][0]\n",
    "    plt.imshow(inp.squeeze(0))\n",
    "    print(inp.shape)\n",
    "    p = 28\n",
    "    class monk:\n",
    "        patch_size = p\n",
    "    out = ShiftedPatchDataset._shifted_patch(monk(), inp)\n",
    "    \n",
    "    N = out.size(0)\n",
    "    fig = plt.figure()\n",
    "    C = 1\n",
    "\n",
    "    out = out.reshape(N, 5, C, p, p)\n",
    "\n",
    "    fig = plt.figure(figsize=(5, N))\n",
    "    for i in range(N):\n",
    "        for j in range(5):\n",
    "            ax = fig.add_subplot(N, 5, i * 5 + j + 1)\n",
    "            ax.axis(\"off\")\n",
    "            ax.imshow(out[i, j, 0].cpu(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a58ebf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_preprocessing():\n",
    "    p = 7\n",
    "    train_data = PatchDataset(raw_train_data, p)\n",
    "    train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "    x_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "    x = x_batch[0]\n",
    "    print(y_batch[0])\n",
    "    n = int(x.size(0) ** (1/2))\n",
    "    fig = plt.figure()\n",
    "    for i in range(x.size(0)):\n",
    "        fig.add_subplot(n, n, i+1)\n",
    "        plt.imshow(x[i].reshape(p, p) , cmap='gray', vmin=0, vmax=1)\n",
    "        plt.axis('off')\n",
    "\n",
    "test_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687e2a1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c30668",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "196deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenization(nn.Module):\n",
    "    def __init__(self, patch_dim, emb_dim):\n",
    "        \"\"\"\n",
    "        Tokenization and add CLS token\n",
    "        (B, N, P*P*C) -> (B, N+1, d)\n",
    "        patch_dim = P*P*C\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(patch_dim, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.projection(x) # (B, N, d)\n",
    "        cls_token = self.cls_token.expand(x.size(0), -1, -1) # (B, 1, d)\n",
    "        pos_emb = self.pos_emb.expand(x.size(0), x.size(1) + 1, -1)\n",
    "        return torch.cat((cls_token, x), dim = 1) + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242e7b8",
   "metadata": {},
   "source": [
    "### Shifted Patch Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "88e7a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPT(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, emb_dim):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa518598",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "880f3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    (B, N+1, d) -> (B, N+1, d_v)\n",
    "    d_q = d_k = d_v = d here\n",
    "    note: d_q must equal to d_k in self-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V = nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        R = self.Q(x) @ self.K(x).transpose(-1, -2)\n",
    "        # (B, N+1, d) @ (B, d, N+1) -> (... N+1, N+1)\n",
    "        #      Q             K               Q    K\n",
    "        SA = (\n",
    "            F.softmax(R / math.sqrt(self.emb_dim), -1) # soft max on K\n",
    "            @ self.V(x))\n",
    "        return SA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1809",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fcac9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim, dropout_p = 0.0):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(emb_dim, 4, batch_first=True, dropout=dropout_p)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(emb_dim*2, emb_dim),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.attention(x, x, x, need_weights=False)[0] # get output only\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f09d5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html#using-nn-sequential\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "03f50eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, d_model):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Tokenization(patch_size**2, d_model), # C == 1 in MNIST\n",
    "            Transformer(d_model, 0.6),\n",
    "            Transformer(d_model, 0.6),\n",
    "            Transformer(d_model, 0.6),\n",
    "            Transformer(d_model, 0.6),\n",
    "            # Head\n",
    "            Lambda(lambda x: x[:,0,:]),\n",
    "            nn.Linear(d_model, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919cbd6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "aad7557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def accuracy(input:Tensor, target:Tensor):\n",
    "    preds = torch.argmax(input, dim=1)\n",
    "    return (preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d0734525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(x_batch)\n",
    "            total_acc += accuracy(preds, y_batch)\n",
    "            total_loss += loss_func(preds, y_batch)\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "def fit(model, optimizer, train_loader, val_loader, epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        loss_t = 0\n",
    "        acc_t = 0\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model(x_batch)\n",
    "            loss = loss_func(preds, y_batch)\n",
    "            loss_t += loss\n",
    "            acc_t += accuracy(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"epoch {epoch+1} loss (train): {loss_t / len(train_loader):.4f}, accuracy (train): {acc_t / len(train_loader):.4f}\", end=\" \")\n",
    "        loss, acc = evaluate(model, val_loader, device)\n",
    "        print(f\"loss: {loss:.4f}, accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "07d5b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1f4d54c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PatchDataset(raw_train_data, patch_size = patch_size)\n",
    "train_data, val_data = random_split(train_data, [50000, 10000], generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "30c7dc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (model): Sequential(\n",
      "    (0): Tokenization(\n",
      "      (projection): Linear(in_features=196, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): Transformer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.6, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (4): Dropout(p=0.6, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Transformer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.6, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (4): Dropout(p=0.6, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Transformer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.6, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (4): Dropout(p=0.6, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Transformer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.6, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (4): Dropout(p=0.6, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Lambda()\n",
      "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Params: 147274\n"
     ]
    }
   ],
   "source": [
    "model = ViT(\n",
    "    patch_size = patch_size,\n",
    "    d_model = 64\n",
    ")\n",
    "\n",
    "print(next(model.named_modules())[1])\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d86bb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 196])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=bs*2, shuffle=True)\n",
    "\n",
    "inp  = next(iter(train_loader))[0]\n",
    "print(inp.shape)\n",
    "\n",
    "out = model(inp)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "epochs = 100\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "fit(model, optimizer, train_loader, val_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4c4dd",
   "metadata": {},
   "source": [
    "Save & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ab6de",
   "metadata": {},
   "source": [
    "- ViT\n",
    "    - 準確率 97% 左右, best: 0.9791 (80 epochs)\n",
    "    - Architecture:\n",
    "        - Transformer block x4\n",
    "            - MHSA, `num_head = 4`\n",
    "            - MLP\n",
    "                1. `Linear(emb_dim, emb_dim*2)`,\n",
    "                2. `GELU`,\n",
    "                3. `Linear(emb_dim*2, emb_dim)`\n",
    "\n",
    "    - Hyper Parameters:\n",
    "        - `patch_size = 14` 收斂速度較 28 快, <= 14 準確率顯著將低\n",
    "        - `d_model = 64` 訓練效果最好\n",
    "        - `batch_size = 32`\n",
    "        - `dropout_p = 0.6`\n",
    "        - `leaning_rate = 0.03`\n",
    "    - Trainable Params: 147274"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
