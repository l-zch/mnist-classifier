{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b212267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0cdf7",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c38b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dirs = [Path(\"./\"), Path(\"../\")]\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    # 0.1307 is the mean of the MNIST dataset, 0.3081 is the standard deviation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)), \n",
    "])\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    if (data_dir / \"MNIST\").exists():\n",
    "        raw_train_data = datasets.MNIST(data_dir, train=True, transform=tf)\n",
    "        raw_test_data = datasets.MNIST(data_dir, train=False, transform=tf)\n",
    "        break\n",
    "else:\n",
    "    raw_train_data = datasets.MNIST(\"./\", train=True, download=True, transform=tf)\n",
    "    raw_test_data = datasets.MNIST(\"./\", train=False, download=True, transform=tf)\n",
    "    \n",
    "\n",
    "g = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1965999",
   "metadata": {},
   "source": [
    "### Patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d589a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    divide image into non-overlapping patches\n",
    "    (B, C, H, W) -> (B, N, P*P*C) where N = (H/P)*(W/P)\n",
    "    note: H and W must be divisible by P\n",
    "    \"\"\"\n",
    "    # if x.size(-1) % patch_size != 0 or x.size(-2) % patch_size != 0:\n",
    "    #     raise ValueError(\"Height and Width must be divisible by patch_size\")\n",
    "    N = (x.size(-2)//patch_size) * (x.size(-1) // patch_size)\n",
    "    x = x.unfold(2, patch_size,patch_size).unfold(3, patch_size,patch_size)  # (B, C, H/P, W/P, P, P)\n",
    "    x = x.permute(0, 2, 3, 1, 4, 5)  # (B, H/P, W/P, C, P, P)\n",
    "\n",
    "    return x.reshape(x.size(0), N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa23644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_patch(x: torch.Tensor, patch_size: int):\n",
    "    \"\"\"\n",
    "    Concat shifted images feature -> Patch features -> Patch flattening\n",
    "    (B, C, H, W) -> (B, N, P*P*C*5)\n",
    "    \"\"\"\n",
    "    # Shifting and concat\n",
    "    # (B, C, H, W) -> (B, C, H, W, 5)\n",
    "    p = patch_size / 2\n",
    "    translates = [[0, 0], [p, p],[p,-p],[-p,-p],[-p,p]]\n",
    "    shifted = torch.stack(\n",
    "        [ v2.functional.affine(x, 0, translate, 1, [0.0], 0) for translate in translates ],\n",
    "        dim = 1\n",
    "    )\n",
    "\n",
    "    # Patch Features & flattening\n",
    "    N = (shifted.size(-2)//patch_size) * (shifted.size(-1) // patch_size)\n",
    "\n",
    "    # (B, 5, C, H, W) -> (B, N, P*P*C*5)\n",
    "    x = shifted.unfold(3,patch_size,patch_size).unfold(4,patch_size,patch_size) # (B, 5, C, H/P, W/P, P, P)\n",
    "    x = x.permute(0, 3, 4, 1, 2, 5, 6) # (B, H/P, W/P, 5, C, P, P)\n",
    "\n",
    "    return x.reshape(x.size(0), N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43621f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_shift_patch():\n",
    "    inp = raw_train_data[0][0]\n",
    "    plt.imshow(inp.squeeze(0), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    p = 14\n",
    "    class monk:\n",
    "        patch_size = p\n",
    "    out = shifted_patch(inp.unsqueeze(0), p)\n",
    "    \n",
    "    N = out.size(1)\n",
    "    fig = plt.figure()\n",
    "    C = 1\n",
    "\n",
    "    out = out.reshape(N, 5, C, p, p)\n",
    "\n",
    "    fig = plt.figure(figsize=(5, N))\n",
    "    for i in range(N):\n",
    "        for j in range(5):\n",
    "            ax = fig.add_subplot(N, 5, i * 5 + j + 1)\n",
    "            if i == 0:\n",
    "                ax.set_title((\"$x$\",\"$S^1$\",\"$S^2$\",\"$S^3$\",\"$S^4$\")[j])\n",
    "            ax.axis(\"off\")\n",
    "            ax.imshow(out[i, j, 0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "test_shift_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a58ebf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_patch():\n",
    "    inp = raw_train_data[0][0]\n",
    "\n",
    "    plt.imshow(inp.squeeze(0), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    p = 14\n",
    "    out = patch(inp.unsqueeze(0), p)\n",
    "\n",
    "    N = out.size(1)\n",
    "    fig = plt.figure()\n",
    "    C = 1\n",
    "    out = out.reshape(N, C, p, p)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(N):\n",
    "        ax = fig.add_subplot(int(N ** (1/2)), int(N ** (1/2)), i+1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.imshow(out[i,0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "\n",
    "test_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c30668",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenization(nn.Module):\n",
    "    def __init__(self, patch_dim, emb_dim):\n",
    "        \"\"\"\n",
    "        Tokenization and add CLS token\n",
    "        (B, N, P*P*C) -> (B, N+1, d)\n",
    "        patch_dim = P*P*C\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(patch_dim, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.projection(x) # (B, N, d)\n",
    "        cls_token = self.cls_token.expand(x.size(0), -1, -1) # (B, 1, d)\n",
    "        pos_emb = self.pos_emb.expand(x.size(0), x.size(1) + 1, -1)\n",
    "        return torch.cat((cls_token, x), dim = 1) + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242e7b8",
   "metadata": {},
   "source": [
    "### Shifted Patch Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bfddc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Shifted Patch Tokenization\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, emb_dim):\n",
    "        \"\"\"\n",
    "        (B, N, P*P*C*5) -> (B, N+1, d)\n",
    "        patch_dim = P*P*C*5\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        patch_dim = patch_size * patch_size * 1 * 5\n",
    "\n",
    "        self.norm = nn.LayerNorm(patch_dim)\n",
    "        self.projection = nn.Linear(patch_dim, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = shifted_patch(x, self.patch_size)\n",
    "        x = self.projection(self.norm(x)) # (B, N, d)\n",
    "\n",
    "        # Embedding Layer\n",
    "        cls_token = self.cls_token.expand(x.size(0), -1, -1) # (B, 1, d)\n",
    "        pos_emb = self.pos_emb.expand(x.size(0), x.size(1) + 1, -1)\n",
    "        x = torch.cat((cls_token, x), dim = 1) + pos_emb\n",
    "        \n",
    "        return x\n",
    "        # Pooling Layer\n",
    "        # (B, N+1, d) -> ((B, N, d) ; (B, 1, d)) -> ((B, H/P, W/P, d); (B, ,, d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa518598",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880f3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    (B, N+1, d) -> (B, N+1, d_v)\n",
    "    d_q = d_k = d_v = d here\n",
    "    note: d_q must equal to d_k in self-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V = nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        R = self.Q(x) @ self.K(x).transpose(-1, -2)\n",
    "        # (B, N+1, d) @ (B, d, N+1) -> (... N+1, N+1)\n",
    "        #      Q             K               Q    K\n",
    "        SA = (\n",
    "            F.softmax(R / math.sqrt(self.emb_dim), -1) # soft max on K\n",
    "            @ self.V(x))\n",
    "        return SA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1809",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcac9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim, dropout_p = 0.0):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(emb_dim, 4, batch_first=True, dropout=dropout_p)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(emb_dim*2, emb_dim),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.attention(x, x, x, need_weights=False)[0] # get output only\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaee708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSA(nn.Module):\n",
    "    \"\"\"\n",
    "    Locality Self-Attention (Multi-head)\n",
    "    (B, N+1, d) -> (B, N+1, d_v)\n",
    "    d_q = d_k = d_v = d here\n",
    "    note: d_q must equal to d_k in self-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, num_heads, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V = nn.Linear(emb_dim, emb_dim)\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        self.t = nn.Parameter(torch.tensor(self.head_dim ** (1/2))) # learnable temperature\n",
    "        self.proj = nn.Linear(emb_dim, emb_dim)\n",
    "        self.atten_drop = nn.Dropout(dropout)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # (B, N, d) -> (B, N, num_heads, head_dim) -> (B, num_heads, N, head_dim)\n",
    "        head_dim = self.head_dim\n",
    "        B, N, _ = x.size()\n",
    "        q = self.Q(x).reshape(B, N, self.num_heads, head_dim).transpose(1, 2)\n",
    "        k = self.K(x).reshape(B, N, self.num_heads, head_dim).transpose(1, 2)\n",
    "        v = self.V(x).reshape(B, N, self.num_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # R: (B, num_heads, N+1, N+1)\n",
    "        R = (q @ k.transpose(-1, -2)) / self.t\n",
    "        # Diagonal Masking\n",
    "        R.diagonal(2, 3).fill_(float(\"-inf\"))\n",
    "\n",
    "        # atten_heads: (B, num_heads, N+1, head_dim)\n",
    "        atten_heads = (\n",
    "            F.softmax(R, dim = -1) # soft max on K\n",
    "            @ v)\n",
    "        atten_heads = self.atten_drop(atten_heads)\n",
    "\n",
    "        # (B, num_heads, N+1, head_dim) -> (B, N+1, emb_dim)\n",
    "        atten_heads = atten_heads.transpose(1, 2).reshape(B, N, self.emb_dim)\n",
    "        out = self.proj(atten_heads)\n",
    "        out = self.proj_drop(out)\n",
    "        return out # combine heads by projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519fb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSATransformer(nn.Module):\n",
    "    def __init__(self, emb_dim, dropout_p = 0.0):\n",
    "        super().__init__()\n",
    "        self.attention = LSA(emb_dim, 4, dropout = dropout_p)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(emb_dim*2, emb_dim),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.attention(x)\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09d5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html#using-nn-sequential\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5ecd3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f50eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, d_model):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Lambda(lambda x: patch(x, patch_size)),\n",
    "            # Tokenization(patch_size**2, d_model), # C == 1 in MNIST\n",
    "            SPT(patch_size, d_model),\n",
    "            Transformer(d_model, 0.6),\n",
    "            Transformer(d_model, 0.6),\n",
    "            Transformer(d_model, 0.6),\n",
    "            Transformer(d_model, 0.6),\n",
    "            # Head\n",
    "            Lambda(lambda x: x[:,0,:]),\n",
    "            nn.Linear(d_model, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "class SL_ViT(nn.Module):\n",
    "    def __init__(self, patch_size, d_model):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Lambda(lambda x: patch(x, patch_size)),\n",
    "            Tokenization(patch_size**2, d_model), # C == 1 in MNIST\n",
    "            # SPT(patch_size, d_model),\n",
    "            LSATransformer(d_model, 0.6),\n",
    "            LSATransformer(d_model, 0.6),\n",
    "            LSATransformer(d_model, 0.6),\n",
    "            LSATransformer(d_model, 0.6),\n",
    "            # Head\n",
    "            Lambda(lambda x: x[:,0,:]),\n",
    "            nn.Linear(d_model, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919cbd6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30c7dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_runner import TrainRunner\n",
    "runner = TrainRunner(raw_train_data, Path(\"./run/experiment4\"))\n",
    "# runner.quick_validation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e8a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SL_ViT(\n",
    "    patch_size = 14,\n",
    "    d_model = 256\n",
    ")\n",
    "runner.train(model, lr=0.03, bs=32, epochs=60, name=\"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1576eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    patch_size = 14,\n",
    "    d_model = 64\n",
    ")\n",
    "runner.train(model, lr=0.03, bs=32, epochs=60, name=\"baseline\")\n",
    "\n",
    "# model = ViT(\n",
    "#     patch_size = 28,\n",
    "#     d_model = 64\n",
    "# )\n",
    "# runner.train(model, lr=0.03, bs=32, epochs=60, name=\"ps_28\")\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "    patch_size = 7,\n",
    "    d_model = 64\n",
    ")\n",
    "runner.train(model, lr=0.03, bs=32, epochs=60, name=\"ps_7\")\n",
    "\n",
    "# model = ViT(\n",
    "#     patch_size = 4,\n",
    "#     d_model = 64\n",
    "# )\n",
    "# runner.train(model, lr=0.03, bs=32, epochs=60, name=\"ps_4\")\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "    patch_size = 14,\n",
    "    d_model = 256\n",
    ")\n",
    "runner.train(model, lr=0.03, bs=32, epochs=60, name=\"d_256\")\n",
    "\n",
    "model = ViT(\n",
    "    patch_size = 14,\n",
    "    d_model = 128\n",
    ")\n",
    "runner.train(model, lr=0.03, bs=32, epochs=60, name=\"d_128\")\n",
    "\n",
    "model = ViT(\n",
    "    patch_size = 14,\n",
    "    d_model = 32\n",
    ")\n",
    "runner.train(model, lr=0.03, bs=32, epochs=60, name=\"d_32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4c4dd",
   "metadata": {},
   "source": [
    "Save & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ab6de",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "- baseline\n",
    "    - Architecture:\n",
    "        - Patch Embedding\n",
    "        - Transformer block x4\n",
    "            - MHSA, `num_head = 4`\n",
    "            - MLP\n",
    "                1. `Linear(emb_dim, emb_dim*2)`,\n",
    "                2. `GELU`,\n",
    "                3. `Linear(emb_dim*2, emb_dim)`\n",
    "        - MLP Head\n",
    "\n",
    "    - Hyper Parameters:\n",
    "        - `patch_size = 14`\n",
    "        - `d_model = 64`\n",
    "        - `batch_size = 64`\n",
    "        - `dropout_p = 0.6`\n",
    "        - `leaning_rate = 0.03`\n",
    "    \n",
    "- different `batch_size` (**64**, 32, 16, 8) \n",
    "  <div align=\"left\">\n",
    "    <img src=\"./results/experiment1/1_1.png\" width=\"45%\">\n",
    "    <img src=\"./results/experiment1/1_2.png\" width=\"45%\">\n",
    "  </div>\n",
    "\n",
    "  - 32, 16, 8 準確率相近，並且都比 64 高且穩定\n",
    "  - 選擇 32 作為後續實驗的 batch_size\n",
    "\n",
    "- different `patch_size` (**14**, 28, 7, 4) \n",
    "  <div align=\"left\">\n",
    "    <img src=\"./results/experiment1/2_1.png\" width=\"45%\">\n",
    "    <img src=\"./results/experiment1/2_2.png\" width=\"45%\">\n",
    "  </div>\n",
    "\n",
    "    - 14 準確率較 28 高，耗時也比較短\n",
    "    - 7、4 欠擬合\n",
    "\n",
    "- different `d_model` (**64**, 32, 128) \n",
    "  <div align=\"left\">\n",
    "    <img src=\"./results/experiment1/3_1.png\" width=\"45%\">\n",
    "  </div>\n",
    "\n",
    "  - 128 效果最好\n",
    "    \n",
    "- Conclusion:\n",
    "  - `batch_size = 32` 準確率最佳\n",
    "  - `patch_size = 14` 準確率最佳\n",
    "  - `d_model` 和準確率成正相關\n",
    "\n",
    "### Experiment 2\n",
    "- baseline\n",
    "  - `batch_size = 32`\n",
    "  - 其餘同 Experiment 1\n",
    "\n",
    "- different `d_model` (**64**, 32, 128, 256) \n",
    "  - 左圖: acc (val), 右圖: loss (train & val)\n",
    "  <div align=\"left\">\n",
    "    <img src=\"./results/experiment2/1_1.png\" width=\"45%\">\n",
    "    <img src=\"./results/experiment2/1_2.png\" width=\"45%\">\n",
    "  </div>\n",
    "\n",
    "  - 256 最佳，但在 epoch 40 開始過擬合\n",
    "\n",
    "\n",
    "- different `patch_size` (**14**, 28, 7, 4) \n",
    "  - 左圖: acc (val), 右圖: loss (train)\n",
    "  <div align=\"left\">\n",
    "    <img src=\"./results/experiment2/2_1.png\" width=\"45%\">\n",
    "    <img src=\"./results/experiment2/2_2.png\" width=\"45%\">\n",
    "  </div>\n",
    "\n",
    "  - 14 依然優於 28\n",
    "  - 7，4 欠擬合 \n",
    "\n",
    "- Conclusion:\n",
    "  - `patch_size = 14` 準確率最佳\n",
    "  - `d_model = 256` 準確率最佳，但有過擬合風險\n",
    "\n",
    "### Experiment 3\n",
    "比較將 Patch Embedding 換成 *Shifted Patch Tokenization* 的效果\n",
    "\n",
    "Shifted Patch Tokenization (SPT) 來自論文 [Vision Transformer for Small-Size Datasets](https://arxiv.org/abs/2112.13492)\n",
    "\n",
    "- `patch_size = 7` SPT 提升效果明顯:\n",
    "\n",
    "  | `d_model` | Acc (Normal) | Acc (SPT) | Improvement |\n",
    "  |------------|------------------|----------------|--------------|\n",
    "  | 64 | 0.9503           | 0.9746         |   0.0243   |\n",
    "  - 推測是 SPT 補充了小 patch size 不足的空間資訊\n",
    "\n",
    "- `patch_size = 14` 表現均下降\n",
    "\n",
    "  | `d_model` | Acc (Normal) | Acc (SPT) | Improvement |\n",
    "  |------------|------------------|----------------|--------------|\n",
    "  | 32 | 0.9756           | 0.9712         | -0.0044      |\n",
    "  | 64       | 0.9792           | 0.9756         | -0.0036      |\n",
    "  | 128      | 0.9827           | 0.9763         | -0.0064      |\n",
    "  | 256      | 0.9837           | 0.9799         | -0.0038      |\n",
    "\n",
    "  - d_model = 32, 64, 256 :\n",
    "    過擬合\n",
    "\n",
    "  - d_model = 128:\n",
    "    訓練不佳\n",
    "  \n",
    "  - 深色線: loss (train)\n",
    "    淺色線: loss (val)\n",
    "    <div align=\"left\">\n",
    "      <img src=\"./results/experiment3/1_1.png\" width=\"45%\">\n",
    "      <img src=\"./results/experiment3/1_2.png\" width=\"45%\">\n",
    "      <img src=\"./results/experiment3/1_3.png\" width=\"45%\">\n",
    "      <img src=\"./results/experiment3/1_4.png\" width=\"45%\">\n",
    "    </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
