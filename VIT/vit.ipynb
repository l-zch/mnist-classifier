{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8b212267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0cdf7",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "38c38b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dirs = [Path(\"./\"), Path(\"../\")]\n",
    "\n",
    "tf = transforms.Compose([\n",
    "    # 0.1307 is the mean of the MNIST dataset, 0.3081 is the standard deviation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)), \n",
    "])\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    if (data_dir / \"MNIST\").exists():\n",
    "        train_data = datasets.MNIST(data_dir, train=True, transform=tf)\n",
    "        test_data = datasets.MNIST(data_dir, train=False, transform=tf)\n",
    "        break\n",
    "else:\n",
    "    train_data = datasets.MNIST(\"./\", train=True, download=True, transform=tf)\n",
    "    test_data = datasets.MNIST(\"./\", train=False, download=True, transform=tf)\n",
    "    \n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_data, val_data = random_split(train_data, [50000, 10000], generator=g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687e2a1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1965999",
   "metadata": {},
   "source": [
    "### Patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "61d589a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch(x: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    divide image into non-overlapping patches\n",
    "    (B, C, H, W) -> (B, N, P*P*C) where N = (H/P)*(W/P)\n",
    "    note: H and W must be divisible by P\n",
    "    \"\"\"\n",
    "    if x.size(2) % patch_size != 0 or x.size(3) % patch_size != 0:\n",
    "        raise ValueError(\"Height and Width must be divisible by patch_size\")\n",
    "    x = x.unfold(2,patch_size,patch_size).unfold(3,patch_size,patch_size)\n",
    "    x = x.reshape(x.size(0), -1, patch_size * patch_size)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "96b52670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 49, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 49 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = 4\n",
    "inp  = train_data[0][0].unsqueeze(0)\n",
    "print(inp.shape)\n",
    "out = patch(inp, p)\n",
    "print(out.shape)\n",
    "\n",
    "plt.imshow(inp.squeeze(0,1), cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(out.size(1)):\n",
    "    fig.add_subplot(inp.size(2)// p,inp.size(2)//p,i+1)\n",
    "    plt.imshow(out.squeeze(0)[i].reshape(p, p) , cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c30668",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "196deb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenization(nn.Module):\n",
    "    def __init__(self, patch_dim, emb_dim):\n",
    "        \"\"\"\n",
    "        Tokenization and add CLS token\n",
    "        (B, N, P*P*C) -> (B, N+1, d)\n",
    "        p_size = P*P*C\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(patch_dim, emb_dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.projection(x).squeeze(-2) # (B, N, 1, d) -> (B, N, d)\n",
    "        cls_token = self.cls_token.expand(x.size(0), -1, -1) # (B, 1, d)\n",
    "        pos_emb = self.pos_emb.expand(x.size(0), x.size(1) + 1, -1)\n",
    "        return torch.cat((cls_token, x), dim = 1) + pos_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa518598",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "880f3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    (B, N+1, d) -> (B, N+1, d_v)\n",
    "    d_q = d_k = d_v = d here\n",
    "    note: d_q must equal to d_k in self-attention\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.V = nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        R = self.Q(x) @ self.K(x).transpose(-1, -2)\n",
    "        # (B, N+1, d) @ (B, d, N+1) -> (... N+1, N+1)\n",
    "        #      Q             K               Q    K\n",
    "        SA = (\n",
    "            F.softmax(R / math.sqrt(self.emb_dim), -1) # soft max on K\n",
    "            @ self.V(x))\n",
    "        return SA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1809",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "fcac9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttention(emb_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim*2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim*2, emb_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x)) # residual connect\n",
    "        return x + self.mlp(self.norm2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "f09d5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://docs.pytorch.org/tutorials/beginner/nn_tutorial.html#using-nn-sequential\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "03f50eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, d_model):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            Lambda(lambda x: patch(x, patch_size)),\n",
    "            Tokenization(patch_size**2, d_model), # C == 1 in MNIST\n",
    "            Transformer(d_model),\n",
    "            Transformer(d_model),\n",
    "            Transformer(d_model),\n",
    "            # Head\n",
    "            Lambda(lambda x: x[:,0,:]),\n",
    "            nn.Linear(d_model, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919cbd6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "aad7557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from torch import Tensor\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def accuracy(input:Tensor, target:Tensor):\n",
    "    preds = torch.argmax(input, dim=1)\n",
    "    return (preds == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d0734525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model.forward(x_batch)\n",
    "            total_acc += accuracy(preds, y_batch)\n",
    "            total_loss += loss_func(preds, y_batch)\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n",
    "\n",
    "def fit(model, optimizer, train_loader, val_loader, epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        loss_t = 0\n",
    "        acc_t = 0\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            preds = model.forward(x_batch)\n",
    "            loss = loss_func(preds, y_batch)\n",
    "            loss_t += loss\n",
    "            acc_t += accuracy(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"epoch {epoch+1} loss (train): {loss_t / len(train_loader):.4f}, accuracy (train): {acc_t / len(train_loader):.4f}\", end=\" \")\n",
    "        loss, acc = evaluate(model, val_loader, device)\n",
    "        print(f\"loss: {loss:.4f}, accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    patch_size = 2,\n",
    "    d_model = 128\n",
    ")\n",
    "\n",
    "print(next(model.named_modules())[1])\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=bs*2, shuffle=True)\n",
    "\n",
    "inp  = next(iter(train_loader))[0]\n",
    "print(inp.shape)\n",
    "out = model(inp)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.03\n",
    "epochs = 10\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.0)\n",
    "\n",
    "fit(model, optimizer, train_loader, val_loader, epochs, 'mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4c4dd",
   "metadata": {},
   "source": [
    "Save & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd6e8cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
